{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python-headless\n"
      ],
      "metadata": {
        "id": "D8JGUCR8QEpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rAZDAd7cKb_"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "convolutional neural network (CNN) for Handwritten Digit Recognition\n",
        "\"\"\"\n",
        "\n",
        "# Larger CNN for the MNIST Dataset\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy  # For numerical operations\n",
        "from tensorflow import keras\n",
        "from keras.datasets import mnist  # For loading the MNIST dataset\n",
        "from keras.models import Sequential  # For building a sequential neural network model\n",
        "from keras.layers import Dense  # For creating fully connected (dense) layers\n",
        "from keras.layers import Dropout  # For adding dropout regularization\n",
        "from keras.layers import Flatten  # For flattening 2D matrices into 1D vectors\n",
        "from keras.layers import Conv2D  # For adding 2D convolutional layers\n",
        "from keras.layers import MaxPooling2D  # For adding max pooling layers\n",
        "from tensorflow.keras.utils import to_categorical  # for utility functions, including one-hot encoding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  For image prediction\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import load_model\n",
        "from google.colab import drive  # Only needed in Colab"
      ],
      "metadata": {
        "id": "Gwb0UGQSNHjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# Load the MNIST dataset\n",
        "# The dataset consists of images of handwritten digits (0-9) and their corresponding labels\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape the data to [samples][width][height] [channels]for CNN input\n",
        "# Each image is 28x28 pixels, and we add a single channel (1) for grayscale\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')\n",
        "\n",
        "# Normalize inputs from 0-255 to 0-1\n",
        "# Pixel values are scaled to the range [0, 1] to improve model training performance\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "# One-hot encode the output labels\n",
        "# The labels are integers (0-9), converted to one-hot encoded vectors for multi-class classification\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "num_classes = y_test.shape[1]  # Number of output classes (10 digits in total)\n",
        "\n",
        "# Define the larger CNN model as a function\n",
        "def larger_model():\n",
        "    # Create a sequential model\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add a 2D convolutional layer with 30 filters, 5x5 kernel, and ReLU activation\n",
        "    # Input shape for the first layer must match the reshaped image data (1 channel, 28x28)\n",
        "    model.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
        "\n",
        "    # Add a max pooling layer with a 2x2 pooling window\n",
        "    # This reduces the spatial dimensions by half, helping to downsample the image\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Add a second convolutional layer with 15 filters and 3x3 kernel with ReLU activation\n",
        "    model.add(Conv2D(15, (3, 3), activation='relu'))\n",
        "\n",
        "    # Add another max pooling layer with a 2x2 pooling window\n",
        "    # Further reduces spatial dimensions\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Add a dropout layer with 20% dropout rate to prevent overfitting\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Flatten the feature maps into a 1D vector for input into the fully connected layers\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Add a fully connected (dense) layer with 128 neurons and ReLU activation\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "\n",
        "    # Add another fully connected layer with 50 neurons and ReLU activation\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "\n",
        "    # Add the output layer with 'num_classes' neurons and softmax activation\n",
        "    # Softmax activation is used to output probabilities for each class (0-9)\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    # Categorical crossentropy is the loss function for multi-class classification\n",
        "    # Adam optimizer is used for training, and accuracy is tracked as a performance metric\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build the model by calling the larger_model function\n",
        "model = larger_model()\n",
        "\n",
        "# Train the model\n",
        "\n",
        "# Fit the model on the training data\n",
        "# We specify validation data (X_test, y_test) for evaluating the model's accuracy on unseen data during training\n",
        "# Epochs = 10 means the model will iterate over the data 10 times\n",
        "# Batch size = 200 means the model will update weights after every 200 samples\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
        "\n",
        "# Evaluate model\n",
        "\n",
        "# Final evaluation of the model on the test data\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "# Print the error rate of the model on the test data\n",
        "# Baseline error is calculated as (100 - accuracy), where accuracy is given as a percentage\n",
        "print(\"Large CNN Error: %.2f%%\" % (100 - scores[1] * 100))\n"
      ],
      "metadata": {
        "id": "aKuTL3qANKei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save model\n",
        "model.save('/content/drive/MyDrive/digit_model.h5')  # Save model to Google Drive\n",
        "print(\"âœ… Model saved to Google Drive.\")\n"
      ],
      "metadata": {
        "id": "WAXHrG3JOPWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #  Predict digit from image\n",
        "# Mount Google Drive (if using Colab)\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "PGGOOJY_V1Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Path to the digit image in your Drive\n",
        "image_path = '/content/sample_data/number.jpeg'\n",
        "\n"
      ],
      "metadata": {
        "id": "msDIDIWmV8F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Load image in grayscale\n",
        "img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "if img is None:\n",
        "    print(\" Error: Image not found. Check the path.\")\n",
        "else:\n",
        "    # Resize to 28x28\n",
        "    img = cv2.resize(img, (28, 28))\n",
        "\n",
        "    # Invert colors (white background, black digit)\n",
        "    img = cv2.bitwise_not(img)\n",
        "\n",
        "    # Show the image\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.title(\"Input Image\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Normalize and reshape\n",
        "    img = img.astype('float32') / 255.0\n",
        "    img = img.reshape(1, 28, 28, 1)\n",
        "\n",
        "    # Load the model (again, to show how it works after saving)\n",
        "    model = load_model('/content/drive/MyDrive/digit_model.h5')\n",
        "\n",
        "    # Predict\n",
        "    prediction = model.predict(img)\n",
        "    predicted_digit = numpy.argmax(prediction)\n",
        "\n",
        "    print(f\" Predicted digit is: {predicted_digit}\")"
      ],
      "metadata": {
        "id": "dxXDUmv-ORCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "MrAH0XtdT8h4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}